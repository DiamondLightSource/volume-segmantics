window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "volume_segmantics", "modulename": "volume_segmantics", "type": "module", "doc": "<h1 id=\"volume-segmantics\">Volume Segmantics</h1>\n\n<p>A toolkit for semantic segmentation of volumetric data using PyTorch deep learning models.</p>\n\n<p>Given a 3d image volume and corresponding dense labels (the segmentation), a 2d model is trained on image slices taken along the x, y, and z axes. The method is optimised for small training datasets, e.g a single $384^3$ pixel dataset. To achieve this, all models use pretrained encoders and image augmentations are used to expand the size of the training dataset.</p>\n\n<p>This work utilises the abilities afforded by the excellent <a href=\"https://github.com/qubvel/segmentation_models.pytorch\">segmentation-models-pytorch</a> library. Also the metrics and loss functions used make use of the hard work done by Adrian Wolny in his <a href=\"https://github.com/wolny/pytorch-3dunet\">pytorch-3dunet</a> repository. </p>\n\n<h2 id=\"installation\">Installation</h2>\n\n<p>At present, the easiest way to install is to create a new conda enviroment or virtualenv with python (ideally &gt;= version 3.8) and pip, activate the envionment and <code>pip install volume-segmantics</code>.</p>\n\n<h2 id=\"configuration-and-command-line-use\">Configuration and command line use</h2>\n\n<p>After installation, two new commands will be available from your terminal whilst your environment is activated, <code>model-train-2d</code> and <code>model-predict-2d</code>.</p>\n\n<p>These commands require access to some settings stored in YAML files. These need to be located in a directory named <code>volseg-settings</code> within the directory where you are running the commands. The settings files can be copied from <a href=\"https://gitlab.diamond.ac.uk/data-analysis/imaging/unet-segmentation/-/tree/packaging/settings\">here</a>. </p>\n\n<p>The file <code>2d_model_train_settings.yaml</code> can be edited in order to change training parameters such as number of epochs, loss functions, evaluation metrics and also model and encoder architectures. The file <code>2d_model_predict_settings.yaml</code> can be edited to change parameters such as the prediction \"quality\" e.g \"low\" quality refers to prediction of the volume segmentation by taking images along a single axis (images in the (x,y) plane). For \"medium\" and \"high\" quality, predictions are done along 3 axes and in 12 directions (3 axes, 4 rotations) respectively, before being combined by maximum probability. </p>\n\n<h3 id=\"for-training-a-2d-model-on-a-3d-image-volume-and-corresponding-labels\">For training a 2d model on a 3d image volume and corresponding labels</h3>\n\n<p>Run the following command. Input files can be in HDF5 or multipage TIFF format.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code>model-train-2d --data path/to/image/data.h5 --labels path/to/corresponding/segmentation/labels.h5\n</code></pre></div>\n\n<p>A model will be trained according to the settings defined in <code>/volseg-settings/2d_model_train_settings.yaml</code> and saved to your working directory. In addition, a figure showing \"ground truth\" segmentation vs model segmentation for some images in the validation set will be saved. </p>\n\n<h5 id=\"for-3d-volume-segmentation-prediction-using-a-2d-model\">For 3d volume segmentation prediction using a 2d model</h5>\n\n<p>Run the following command. Input image files can be in HDF5 or multipage TIFF format.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code>model-predict-2d path/to/model_file.pytorch path/to/data_for_prediction.h5\n</code></pre></div>\n\n<p>The input data will be segmented using the input model following the settings specified in <code>volseg-settings/2d_model_predict_settings.yaml</code>. An HDF5 file containing the segmented volume will be saved to your working directory.</p>\n"}, {"fullname": "volume_segmantics.data", "modulename": "volume_segmantics.data", "type": "module", "doc": "<p></p>\n"}, {"fullname": "volume_segmantics.data.get_2d_training_dataloaders", "modulename": "volume_segmantics.data", "qualname": "get_2d_training_dataloaders", "type": "function", "doc": "<p>Returns 2d training and validation dataloaders with indices split at random\naccording to the percentage split specified in settings.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>image_dir (Path):</strong>  Directory of data images</li>\n<li><strong>label_dir (Path):</strong>  Directory of label images</li>\n<li><strong>settings (SimpleNamespace):</strong>  Settings object</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Tuple[DataLoader, DataLoader]: 2d training and validation dataloaders</p>\n</blockquote>\n", "signature": "(\n    image_dir: pathlib.Path,\n    label_dir: pathlib.Path,\n    settings: types.SimpleNamespace\n) -> Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader]", "funcdef": "def"}, {"fullname": "volume_segmantics.data.get_settings_data", "modulename": "volume_segmantics.data", "qualname": "get_settings_data", "type": "function", "doc": "<p>Creates an object holding settings data if a path to a YAML file or a\ndictionary is given. If the data is None, an empty namespace is returned.</p>\n", "signature": "(data: Union[pathlib.Path, dict, NoneType]) -> types.SimpleNamespace", "funcdef": "def"}, {"fullname": "volume_segmantics.data.TrainingDataSlicer", "modulename": "volume_segmantics.data", "qualname": "TrainingDataSlicer", "type": "class", "doc": "<p>Class that converts 3d data volumes into 2d image slices on disk for\nmodel training.\nSlicing is carried in all of the xy (z), xz (y) and yz (x) planes.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>settings (SimpleNamespace):</strong>  An initialised object with settings data.</li>\n</ul>\n", "bases": "volume_segmantics.data.base_data_manager.BaseDataManager"}, {"fullname": "volume_segmantics.data.TrainingDataSlicer.__init__", "modulename": "volume_segmantics.data", "qualname": "TrainingDataSlicer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    settings,\n    data_vol: Union[str, numpy.ndarray],\n    label_vol: Union[str, numpy.ndarray]\n)", "funcdef": "def"}, {"fullname": "volume_segmantics.data.TrainingDataSlicer.preprocess_labels", "modulename": "volume_segmantics.data", "qualname": "TrainingDataSlicer.preprocess_labels", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "volume_segmantics.data.TrainingDataSlicer.fix_label_classes", "modulename": "volume_segmantics.data", "qualname": "TrainingDataSlicer.fix_label_classes", "type": "function", "doc": "<p>Changes the data values of classes in a segmented volume so that\nthey start from zero.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>seg_classes(list):</strong>  An ascending list of the labels in the volume.</li>\n</ul>\n", "signature": "(self, seg_classes)", "funcdef": "def"}, {"fullname": "volume_segmantics.data.TrainingDataSlicer.output_data_slices", "modulename": "volume_segmantics.data", "qualname": "TrainingDataSlicer.output_data_slices", "type": "function", "doc": "<p>Wrapper method to intitiate slicing data volume to disk.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>data_dir (pathlib.Path):</strong>  The path to the directory where images will be saved.</li>\n</ul>\n", "signature": "(self, data_dir, prefix)", "funcdef": "def"}, {"fullname": "volume_segmantics.data.TrainingDataSlicer.output_label_slices", "modulename": "volume_segmantics.data", "qualname": "TrainingDataSlicer.output_label_slices", "type": "function", "doc": "<p>Wrapper method to intitiate slicing label volume to disk.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>data_dir (pathlib.Path):</strong>  The path to the directory where images will be saved.</li>\n</ul>\n", "signature": "(self, data_dir, prefix)", "funcdef": "def"}, {"fullname": "volume_segmantics.data.TrainingDataSlicer.output_slices_to_disk", "modulename": "volume_segmantics.data", "qualname": "TrainingDataSlicer.output_slices_to_disk", "type": "function", "doc": "<p>Coordinates the slicing of an image volume in the three orthogonal\nplanes to images on disk.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>data_arr (array):</strong>  The data volume to be sliced.</li>\n<li><strong>output_path (pathlib.Path):</strong>  A Path object to the output directory.</li>\n<li><strong>label (bool):</strong>  Whether this is a label volume.</li>\n</ul>\n", "signature": "(self, data_arr, output_path, name_prefix, label=False)", "funcdef": "def"}, {"fullname": "volume_segmantics.data.TrainingDataSlicer.output_im", "modulename": "volume_segmantics.data", "qualname": "TrainingDataSlicer.output_im", "type": "function", "doc": "<p>Converts a slice of data into an image on disk.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>data (numpy.array):</strong>  The data slice to be converted.</li>\n<li><strong>path (str):</strong>  The path of the image file including the filename prefix.</li>\n<li><strong>label (bool):</strong>  Whether to convert values &gt;1 to 1 for binary segmentation.</li>\n</ul>\n", "signature": "(self, data, path, label=False)", "funcdef": "def"}, {"fullname": "volume_segmantics.data.TrainingDataSlicer.delete_image_dir", "modulename": "volume_segmantics.data", "qualname": "TrainingDataSlicer.delete_image_dir", "type": "function", "doc": "<p></p>\n", "signature": "(self, im_dir_path)", "funcdef": "def"}, {"fullname": "volume_segmantics.data.TrainingDataSlicer.clean_up_slices", "modulename": "volume_segmantics.data", "qualname": "TrainingDataSlicer.clean_up_slices", "type": "function", "doc": "<p>Wrapper function that cleans up data and label image slices.</p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "volume_segmantics.model", "modulename": "volume_segmantics.model", "type": "module", "doc": "<p></p>\n"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer", "type": "class", "doc": "<p>Class that utlises 2d dataloaders to train a 2d deep learning model.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li>sampler</li>\n<li>settings</li>\n</ul>\n"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.__init__", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    training_loader,\n    validation_loader,\n    labels: Union[int, dict],\n    settings\n)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.get_model_struc_dict", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.get_model_struc_dict", "type": "function", "doc": "<p></p>\n", "signature": "(self, settings)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.calculate_log_lr_ratio", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.calculate_log_lr_ratio", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.create_model_and_optimiser", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.create_model_and_optimiser", "type": "function", "doc": "<p></p>\n", "signature": "(self, learning_rate, frozen=False)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.freeze_model", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.freeze_model", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.unfreeze_model", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.unfreeze_model", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.count_trainable_parameters", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.count_trainable_parameters", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> int", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.count_parameters", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.count_parameters", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> int", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.get_loss_criterion", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.get_loss_criterion", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.get_eval_metric", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.get_eval_metric", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.train_model", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.train_model", "type": "function", "doc": "<p>Performs training of model for a number of cycles\nwith a learning rate that is determined automatically.</p>\n", "signature": "(self, output_path, num_epochs, patience, create=True, frozen=False)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.load_in_model_and_optimizer", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.load_in_model_and_optimizer", "type": "function", "doc": "<p></p>\n", "signature": "(self, learning_rate, output_path, frozen=False, optimizer=False)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.load_in_weights", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.load_in_weights", "type": "function", "doc": "<p></p>\n", "signature": "(self, output_path, optimizer=False, gpu=True)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.run_lr_finder", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.run_lr_finder", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.lr_finder", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.lr_finder", "type": "function", "doc": "<p></p>\n", "signature": "(self, lr_scheduler, smoothing=0.05, plt_fig=True)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.find_lr_from_graph", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.find_lr_from_graph", "type": "function", "doc": "<p>Calculates learning rate corresponsing to minimum gradient in graph\nof loss vs learning rate.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>lr_find_loss (torch.Tensor):</strong>  Loss values accumulated during training</li>\n<li><strong>lr_find_lr (torch.Tensor):</strong>  Learning rate used for mini-batch</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>float: The learning rate at the point when loss was falling most steeply\n  divided by a fudge factor.</p>\n</blockquote>\n", "signature": "(lr_find_loss: torch.Tensor, lr_find_lr: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.lr_exp_stepper", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.lr_exp_stepper", "type": "function", "doc": "<p>Exponentially increase learning rate as part of strategy to find the\noptimum.\nTaken from\n<a href=\"https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee\">https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee</a></p>\n", "signature": "(self, x)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.create_optimizer", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.create_optimizer", "type": "function", "doc": "<p></p>\n", "signature": "(self, learning_rate)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.create_exponential_lr_scheduler", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.create_exponential_lr_scheduler", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.create_oc_lr_scheduler", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.create_oc_lr_scheduler", "type": "function", "doc": "<p></p>\n", "signature": "(self, num_epochs, lr_to_use)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.create_early_stopping", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.create_early_stopping", "type": "function", "doc": "<p></p>\n", "signature": "(self, output_path, patience, best_score=None)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.train_one_batch", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.train_one_batch", "type": "function", "doc": "<p></p>\n", "signature": "(self, lr_scheduler, batch)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.output_loss_fig", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.output_loss_fig", "type": "function", "doc": "<p></p>\n", "signature": "(self, model_out_path)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dTrainer.output_prediction_figure", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dTrainer.output_prediction_figure", "type": "function", "doc": "<p>Saves a figure containing image slice data for three random images\nfromthe validation dataset along with the corresponding ground truth\nlabel image and corresponding prediction output from the model attached\nto this class instance. The image is saved to the same directory as the\nmodel weights.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>model_path (pathlib.Path):</strong>  Full path to the model weights file,</li>\n<li>this is used to get the directory and name of the model not to</li>\n<li>load and predict.</li>\n</ul>\n", "signature": "(self, model_path)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dPredictor", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dPredictor", "type": "class", "doc": "<p>Class that performs U-Net prediction operations. Does not interact with disk.</p>\n"}, {"fullname": "volume_segmantics.model.VolSeg2dPredictor.__init__", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dPredictor.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, model_file_path: str, settings: types.SimpleNamespace)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dPredictor.get_model_from_trainer", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dPredictor.get_model_from_trainer", "type": "function", "doc": "<p></p>\n", "signature": "(self, trainer)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dPredictor.predict_single_axis", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dPredictor.predict_single_axis", "type": "function", "doc": "<p></p>\n", "signature": "(self, data_vol, output_probs=False, axis=<Axis.Z: 0>)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dPredictor.predict_3_ways_max_probs", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dPredictor.predict_3_ways_max_probs", "type": "function", "doc": "<p></p>\n", "signature": "(self, data_vol)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dPredictor.merge_vols_in_mem", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dPredictor.merge_vols_in_mem", "type": "function", "doc": "<p></p>\n", "signature": "(self, prob_container, label_container)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dPredictor.predict_12_ways_max_probs", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dPredictor.predict_12_ways_max_probs", "type": "function", "doc": "<p></p>\n", "signature": "(self, data_vol)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dPredictor.predict_single_axis_to_one_hot", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dPredictor.predict_single_axis_to_one_hot", "type": "function", "doc": "<p></p>\n", "signature": "(self, data_vol, axis=<Axis.Z: 0>)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dPredictor.predict_3_ways_one_hot", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dPredictor.predict_3_ways_one_hot", "type": "function", "doc": "<p></p>\n", "signature": "(self, data_vol)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2dPredictor.predict_12_ways_one_hot", "modulename": "volume_segmantics.model", "qualname": "VolSeg2dPredictor.predict_12_ways_one_hot", "type": "function", "doc": "<p></p>\n", "signature": "(self, data_vol)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2DPredictionManager", "modulename": "volume_segmantics.model", "qualname": "VolSeg2DPredictionManager", "type": "class", "doc": "<p></p>\n", "bases": "volume_segmantics.data.base_data_manager.BaseDataManager"}, {"fullname": "volume_segmantics.model.VolSeg2DPredictionManager.__init__", "modulename": "volume_segmantics.model", "qualname": "VolSeg2DPredictionManager.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    predictor: volume_segmantics.model.operations.vol_seg_2d_predictor.VolSeg2dPredictor,\n    data_vol: Union[str, numpy.ndarray],\n    settings: types.SimpleNamespace\n)", "funcdef": "def"}, {"fullname": "volume_segmantics.model.VolSeg2DPredictionManager.predict_volume_to_path", "modulename": "volume_segmantics.model", "qualname": "VolSeg2DPredictionManager.predict_volume_to_path", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    output_path: Optional[pathlib.Path],\n    quality: Optional[volume_segmantics.utilities.base_data_utils.Quality] = None\n) -> numpy.ndarray", "funcdef": "def"}, {"fullname": "volume_segmantics.utilities", "modulename": "volume_segmantics.utilities", "type": "module", "doc": "<p></p>\n"}, {"fullname": "volume_segmantics.utilities.get_2d_training_parser", "modulename": "volume_segmantics.utilities", "qualname": "get_2d_training_parser", "type": "function", "doc": "<p>Argument parser for scripts that train a 2d network on a 3d volume.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>argparse.ArgumentParser: An argument parser with the appropriate\n  command line args contained within.</p>\n</blockquote>\n", "signature": "() -> argparse.ArgumentParser", "funcdef": "def"}, {"fullname": "volume_segmantics.utilities.get_2d_prediction_parser", "modulename": "volume_segmantics.utilities", "qualname": "get_2d_prediction_parser", "type": "function", "doc": "<p>Argument parser for scripts that use a 2d network to predict segmenation for a 3d volume.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>argparse.ArgumentParser: An argument parser with the appropriate\n  command line args contained within.</p>\n</blockquote>\n", "signature": "() -> argparse.ArgumentParser", "funcdef": "def"}, {"fullname": "volume_segmantics.utilities.Quality", "modulename": "volume_segmantics.utilities", "qualname": "Quality", "type": "class", "doc": "<p>An enumeration.</p>\n", "bases": "enum.Enum"}, {"fullname": "volume_segmantics.utilities.Quality.LOW", "modulename": "volume_segmantics.utilities", "qualname": "Quality.LOW", "type": "variable", "doc": "<p></p>\n", "default_value": " = <Quality.LOW: 1>"}, {"fullname": "volume_segmantics.utilities.Quality.MEDIUM", "modulename": "volume_segmantics.utilities", "qualname": "Quality.MEDIUM", "type": "variable", "doc": "<p></p>\n", "default_value": " = <Quality.MEDIUM: 3>"}, {"fullname": "volume_segmantics.utilities.Quality.HIGH", "modulename": "volume_segmantics.utilities", "qualname": "Quality.HIGH", "type": "variable", "doc": "<p></p>\n", "default_value": " = <Quality.HIGH: 12>"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();