# Settings for image and model output
data_im_dirname: data # Name of folder that sliced data 2D images will be output to
seg_im_out_dirname: seg # Name of folder that sliced segmentation 2D images with be output to 
model_output_fn: trained2dUnet_model # Suffix for the saved U-net model filename
clip_data: False # Clip and rescale the image data intensities before saving to disk
st_dev_factor: 2.575 # The number of standard deviations from the mean to clip data to
data_hdf5_path: /data # The internal HDF5 path to the image data
seg_hdf5_path: /data # The internal HDF5 path to the label data

# Settings for Unet training
image_size: 256 # size of images in databunch used for training (must be multiple of 32)
downsample: False # If True, data will be downsampled by 2
training_set_proportion: 0.8 # Proportion of images to use the training, rest are used for validation
cuda_device: 0 # The graphics card to use (between 0 and 3 for a machine with 4 GPUs)
num_cyc_frozen: 8 # Number of times to run fit_one_cycle on frozen unet model
num_cyc_unfrozen: 5 # Number of times to run fit_one_cycle on unfrozen unet model
patience: 3 # Number of epoch to wait before early stopping if validation loss does not improve

loss_criterion: "DiceLoss" # Choose from one of [BCEDiceLoss, BCELoss, DiceLoss, GeneralizedDiceLoss, CrossEntropyLoss]
alpha: 0.75 # When BCEDiceLoss selected, weighting for BCELoss
beta: 0.25 # When BCEDiceLoss selected, weighting for DiceLoss
eval_metric: "MeanIoU" # Choose from one of [MeanIoU, GenericAveragePrecision]
pct_lr_inc: 0.3 # the percentage of overall iterations where the LR is increasing
# Parameters for finding learning rate
starting_lr: 1e-6 # Lower bound of learning rate search
end_lr: 50 # Upper Bound of learning rate search
lr_find_epochs: 1 # Number of training epochs for learning rate search
lr_reduce_factor: 500 # Divisor for start and end LR when finding LR on reloaded model

# Parameters to control unet architecture
model:
  # For more details on encoder types please see smp.readthedocs.io
  # choose encoder, those tested so far include the following:
  # ["resnet34", "resnext50_32x4d"]
  encoder_name: "resnet34"
   # use `imagenet` pre-trained weights for encoder initialization
  encoder_weights: "imagenet"
  
