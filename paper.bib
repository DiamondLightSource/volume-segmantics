@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	language = {en},
	number = {3},
	urldate = {2020-05-21},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	pages = {211--252},
}

@inproceedings{perslev_one_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {One {Network} to {Segment} {Them} {All}: {A} {General}, {Lightweight} {System} for {Accurate} {3D} {Medical} {Image} {Segmentation}},
	isbn = {978-3-030-32245-8},
	shorttitle = {One {Network} to {Segment} {Them} {All}},
	doi = {10.1007/978-3-030-32245-8_4},
	abstract = {Many recent medical segmentation systems rely on powerful deep learning models to solve highly specific tasks. To maximize performance, it is standard practice to evaluate numerous pipelines with varying model topologies, optimization parameters, pre- \& postprocessing steps, and even model cascades. It is often not clear how the resulting pipeline transfers to different tasks.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Perslev, Mathias and Dam, Erik Bjørnager and Pai, Akshay and Igel, Christian},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	pages = {30--38},
}

@article{buslaev_albumentations_2020,
	title = {Albumentations: {Fast} and {Flexible} {Image} {Augmentations}},
	volume = {11},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/11/2/125},
	doi = {10.3390/info11020125},
	number = {2},
	journal = {Information},
	author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
	year = {2020},
}

@misc{alvarez-borges_u-net_2022,
	title = {U-{Net} {Segmentation} {Methods} for {Variable}-{Contrast} {XCT} {Images} of {Methane}-{Bearing} {Sand} {Using} {Small} {Training} {Datasets}},
	copyright = {Creative Commons Attribution 4.0 International License},
	url = {http://www.essoar.org/doi/10.1002/essoar.10506807.2},
	doi = {10.1002/essoar.10506807.2},
	abstract = {Methane (CH4) hydrate dissociation and CH4 release are potential geohazards currently investigated using X-ray computed tomography (XCT) imaging in laboratory experiments. Image segmentation constitu},
	language = {EN},
	urldate = {2022-07-25},
	publisher = {Earth and Space Science Open Archive},
	author = {Alvarez-Borges, Fernando Jesus and King, Oliver N. F. and Madhusudhan, B. N. and Connolley, Thomas and Basham, Mark and Ahmed, Sharif I.},
	month = jul,
	year = {2022},
	note = {Section: Soil Science},
}

@article{tun_massively_2021,
	title = {A massively multi-scale approach to characterizing tissue architecture by synchrotron micro-{CT} applied to the human placenta},
	volume = {18},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsif.2021.0140},
	doi = {10.1098/rsif.2021.0140},
	abstract = {Multi-scale structural assessment of biological soft tissue is challenging but essential to gain insight into structure–function relationships of tissue/organ. Using the human placenta as an example, this study brings together sophisticated sample preparation protocols, advanced imaging and robust, validated machine-learning segmentation techniques to provide the first massively multi-scale and multi-domain information that enables detailed morphological and functional analyses of both maternal and fetal placental domains. Finally, we quantify the scale-dependent error in morphological metrics of heterogeneous placental tissue, estimating the minimal tissue scale needed in extracting meaningful biological data. The developed protocol is beneficial for high-throughput investigation of structure–function relationships in both normal and diseased placentas, allowing us to optimize therapeutic approaches for pathological pregnancies. In addition, the methodology presented is applicable in the characterization of tissue architecture and physiological behaviours of other complex organs with similarity to the placenta, where an exchange barrier possesses circulating vascular and avascular fluid spaces.},
	number = {179},
	urldate = {2022-07-21},
	journal = {Journal of The Royal Society Interface},
	author = {Tun, W. M. and Poologasundarampillai, G. and Bischof, H. and Nye, G. and King, O. N. F. and Basham, M. and Tokudome, Y. and Lewis, R. M. and Johnstone, E. D. and Brownbill, P. and Darrow, M. and Chernyavsky, I. L.},
	note = {Publisher: Royal Society},
	keywords = {computed tomography, contrast agent, flow network, human placenta, machine-learning segmentation, spatial statistics},
	pages = {20210140},
	month = jun,
	year = {2021},
}

@misc{Yakubovskiy:2019,
  author = {Pavel Yakubovskiy},
  title = {Segmentation Models Pytorch},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/qubvel/segmentation_models.pytorch}
}

@article{pennington_survos_2022,
	title = {{SuRVoS} 2: {Accelerating} {Annotation} and {Segmentation} for {Large} {Volumetric} {Bioimage} {Workflows} {Across} {Modalities} and {Scales}},
	volume = {10},
	issn = {2296-634X},
	url = {https://www.frontiersin.org/articles/10.3389/fcell.2022.842342},
	doi = {10.3389/fcell.2022.842342},
	abstract = {As sample preparation and imaging techniques have expanded and improved to include a variety of options for larger sized and numbers of samples, the bottleneck in volumetric imaging is now data analysis. Annotation and segmentation are both common, yet difficult, data analysis tasks which are required to bring meaning to the volumetric data. The SuRVoS application has been updated and redesigned to provide access to both manual and machine learning-based segmentation and annotation techniques, including support for crowd sourced data. Combining adjacent, similar voxels (supervoxels) provides a mechanism for speeding up segmentation both in the painting of annotation and by training a segmentation model on a small amount of annotation. The support for layers allows multiple datasets to be viewed and annotated together which, for example, enables the use of correlative data (e.g. crowd-sourced annotations or secondary imaging techniques) to guide segmentation. The ability to work with larger data on high-performance servers with GPUs has been added through a client-server architecture and the Pytorch-based image processing and segmentation server is flexible and extensible, and allows the implementation of deep learning-based segmentation modules. The client side has been built around Napari allowing integration of SuRVoS into an ecosystem for open-source image analysis while the server side has been built with cloud computing and extensibility through plugins in mind. Together these improvements to SuRVoS provide a platform for accelerating the annotation and segmentation of volumetric and correlative imaging data across modalities and scales.},
	journal = {Frontiers in Cell and Developmental Biology},
	author = {Pennington, Avery and King, Oliver N. F. and Tun, Win Min and Ho, Elaine M. L. and Luengo, Imanol and Darrow, Michele C. and Basham, Mark},
	year = {2022},
}

@misc{survos2:2018,
  author = {Pennington, Avery and King, Oliver N. F. and Luengo, Imanol and Basham, Mark},
  title = {SuRVoS2},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/DiamondLightSource/SuRVoS2}
}

@misc{perslev_github_2019,
  author = {Perslev, Mathias and Igel, Christian},
  title = {Multi-Planar U-Net},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/perslev/MultiPlanarUNet}
}

@misc{tekawade_github_2020,
  author = {Tekawade, Aniket and Igel, Christian},
  title = {CTSegNet},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/aniketkt/CTSegNet}
}

@misc{wolny_github_2019,
  author = {Wolny, Adrian},
  title = {pytorch-3dunet},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/wolny/pytorch-3dunet}
}

@misc{chen_rethinking_2017,
	title = {Rethinking {Atrous} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1706.05587},
	doi = {10.48550/arXiv.1706.05587},
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
	urldate = {2022-09-13},
	publisher = {arXiv},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = dec,
	year = {2017},
	note = {arXiv:1706.05587 [cs]
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lee_deepem_2018,
  author = {Lee, Kisuk and Turner, Nicholas L.},
  title = {DeepEM},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/seung-lab/DeepEM}
}

@article{lee2017superhuman,
  author    = {Kisuk Lee and
               Jonathan Zung and
               Peter Li and
               Viren Jain and
               H. Sebastian Seung},
  title     = {Superhuman Accuracy on the {SNEMI3D} Connectomics Challenge},
  journal   = {arXiv preprint arXiv:1706.00120},
  year      = {2017},
}

@article{lin2021pytorch,
  title={PyTorch Connectomics: A Scalable and Flexible Segmentation Framework for EM Connectomics},
  author={Lin, Zudi and Wei, Donglai and Lichtman, Jeff and Pfister, Hanspeter},
  journal={arXiv preprint arXiv:2112.05754},
  year={2021}
}

@article{urakubo_uni-em_2019,
	title = {{UNI}-{EM}: {An} {Environment} for {Deep} {Neural} {Network}-{Based} {Automated} {Segmentation} of {Neuronal} {Electron} {Microscopic} {Images}},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	shorttitle = {{UNI}-{EM}},
	url = {https://www.nature.com/articles/s41598-019-55431-0},
	doi = {10.1038/s41598-019-55431-0},
	language = {en},
	number = {1},
	urldate = {2022-09-14},
	journal = {Scientific Reports},
	author = {Urakubo, Hidetoshi and Bullmann, Torsten and Kubota, Yoshiyuki and Oba, Shigeyuki and Ishii, Shin},
	month = dec,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Neural circuits, Software},
	pages = {19413},
}

@misc{lin_pth_connec_github_2019,
  author = {Lin, Zudi and Lu, Yuhao and Belhamissi, Mourad and Banerjee, Atmadeep and Lauenburg, Leander and Swaroop, K. Krishna and Wei, Donglai and Pfister, Hanspeter},
  title = {PyTorch Connectomics},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/zudi-lin/pytorch_connectomics}
}

@misc{wu_neutorch_2021,
  author = {Wu, Jingpeng},
  title = {neutorch},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/flatironinstitute/neutorch}
}
